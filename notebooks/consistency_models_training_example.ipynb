{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency Models Training Example\n",
    "\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2301.01469-<COLOR>.svg)](https://arxiv.org/abs/2303.01469) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Kinyugo/consistency_models/blob/main/notebooks/consistency_models_training_example.ipynb) [![GitHub Repo stars](https://img.shields.io/github/stars/Kinyugo/consistency_models?style=social) ](https://github.com/Kinyugo/consistency_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“– Introduction\n",
    "\n",
    "[Consistency Models](https://arxiv.org/abs/2303.01469) are a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks.\n",
    "\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "_Learn a model that maps any arbitrary point in the latent space to the initial data point, i.e: if points lie on the same probability flow trajectory they are mapped to the same initial data point._\n",
    "\n",
    "### Contributions\n",
    "\n",
    "- Single step sampling\n",
    "- Zero-shot data editing: inpainting, outpainting e.t.c\n",
    "\n",
    "### Definition\n",
    "\n",
    "Given a diffusion trajectory $x_{\\sigma \\in \\left[\\sigma_{min}, \\sigma_{max}\\right]}$, we define a consistency function $f : \\left(x_{\\sigma}, \\sigma\\right) \\rightarrow x_{\\sigma_{min}}$.\n",
    "\n",
    "We can then train a consistency model $f_{\\theta}\\left(., . \\right)$ to approximate the consistency function. A property of the consistency function is that $f : \\left(x_{\\sigma_{min}}, \\sigma_{min} \\right) \\rightarrow x_{\\sigma_{min}}$. To achieve this we parameterize the consistency model using skip connections as in [[2]](#2)\n",
    "\n",
    "$$\n",
    "f_{\\theta}\\left(x_{\\sigma}, \\sigma \\right) = c_{skip}\\left(\\sigma \\right)x_{\\sigma} + c_{out}\\left(\\sigma \\right)F_{\\theta}\\left(x_{\\sigma}, \\sigma \\right)\n",
    "$$\n",
    "\n",
    "where $c_{skip}\\left(\\sigma_{min} \\right) = 1$ and $c_{out}\\left(\\sigma_{min} \\right) = 0$ and $F_{\\theta}\\left(.,.\\right)$ is the neural network.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "#### Training\n",
    "\n",
    "To train the model we follow the following algorithm:\n",
    "\n",
    "```python\n",
    "for current_training_step in range(total_training_steps):\n",
    "    data = data_distribution()\n",
    "\n",
    "  num_timesteps = timestep_schedule(current_training_step, total_training_steps, initial_timesteps, final_timesteps)\n",
    "  sigmas = karras_schedule(num_timesteps, sigma_min, sigma_max)\n",
    "  timesteps = uniform_distribution(batch_size, start=0, end=num_timesteps-1)\n",
    "  noise = standard_gaussian_noise()\n",
    "\n",
    "  current_sigmas = sigmas[timesteps]\n",
    "  next_sigmas = sigmas[timesteps + 1]\n",
    "\n",
    "  current_noisy_data = data + current_sigmas * noise\n",
    "  next_noisy_data = data + next_sigmas * noise\n",
    "\n",
    "  student_model_prediction = (skip_scaling(next_sigmas, sigma_data, sigma_min) * next_noisy_data \n",
    "                            + output_scaling(next_sigmas, sigma_data, sigma_min) * student_model(next_noisy_data, next_sigmas))\n",
    "\n",
    "  with no_grad():\n",
    "      teacher_model_prediction = (skip_scaling(current_sigmas, sigma_data, sigma_min) * current_noisy_data \n",
    "                                + output_scaling(current_sigmas, sigma_data, sigma_min) * teacher_model(current_noisy_data, next_sigmas))\n",
    "\n",
    "  loss = distance_metric(student_model_prediction, teacher_model_prediction)\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  with no_grad():\n",
    "    current_ema_decay_rate = ema_decay_schedule(current_training_step, initial_ema_decay_rate, initial_timesteps)\n",
    "    teacher_model_params = current_ema_decay_rate * teacher_model_params + (1 - current_ema_decay_rate) * student_model_params\n",
    "\n",
    "```\n",
    "\n",
    "#### Sampling\n",
    "\n",
    "Starting from an initial random noise $\\hat{x}_{\\sigma_{max}} \\sim \\mathcal{N}(0, \\sigma_{max}^2I)$, the consistency model can be used to sample a point in a single step: $\\hat{x}_{\\sigma_{min}} = f_{\\theta}(x_{\\sigma_{max}}, \\sigma_{max})$. For iterative refinement, the following algorithm can be used:\n",
    "\n",
    "```python\n",
    "# Generate an initial sample from the initial random noise\n",
    "sample = (skip_scaling(sigma_max, sigma_data, sigma_min) * x_sigma_max\n",
    "                + output_scaling(sigma_max, sigma_data, sigma_min) * model(x_sigma_max, sigma_max))\n",
    "sample = clamp?(sample)\n",
    "\n",
    "for sigma in sigmas:\n",
    "    noise = standard_gaussian_noise()\n",
    "    noisy_sample = sample + square_root(square(sigma) - square(sigma_min)) * noise\n",
    "    sample = (skip_scaling(sigma, sigma_data, sigma_min) * noisy_sample\n",
    "                + output_scaling(sigma, sigma_data, sigma_min) * model(noisy_sample, sigma))\n",
    "    sample = clamp?(sample)\n",
    "```\n",
    "\n",
    "where `consistency_model` $= f_{\\theta}\\left(.,.\\right)$,\n",
    "`clamp?` is a function that optionally clips values to a given range.\n",
    "\n",
    "### References\n",
    "\n",
    "<a id=\"1\">[1]</a> Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (2023, May 31). Consistency models. arXiv.org. https://arxiv.org/abs/2303.01469 \n",
    "\n",
    "<a id=\"2\">[2]</a> Karras, T., Aittala, M., Aila, T., &amp; Laine, S. (2022, October 11). Elucidating the design space of diffusion-based Generative Models. arXiv.org. https://arxiv.org/abs/2206.00364 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 27 08:10:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:AB:00.0 Off |                  Off |\n",
      "|  0%   40C    P8              21W / 450W |     11MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:E1:00.0 Off |                  Off |\n",
      "|  0%   39C    P8              24W / 450W |     11MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     14040      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     14040      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages Installation\n",
    "\n",
    "> **NOTE:** Restart the runtime if using colab after installing the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2632715329.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install -q -e git+https://github.com/Kinyugo/consistency_models.git#egg=consistency_models\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%pip install -q lightning gdown torchmetrics einops torchinfo lpips --no-cache --upgrade\n",
    "%pip install -q -e git+https://github.com/Kinyugo/consistency_models.git#egg=consistency_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from consistency_models import (\n",
    "    ConsistencySamplingAndEditing,\n",
    "    ConsistencyTraining,\n",
    "    ema_decay_rate_schedule,\n",
    ")\n",
    "from consistency_models.utils import update_ema_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§   Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FnzQLDPs-IlTTEr14YyENKjTYqZfn8mS\n",
      "To: /home/jwgu/git/consistency_models/notebooks/butterflies256.tar.gz\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.13M/9.13M [00:00<00:00, 10.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1FnzQLDPs-IlTTEr14YyENKjTYqZfn8mS  && tar -xf butterflies256.tar.gz # Butterflies Dataset\n",
    "# !gdown 1m1QrNnKJy7hEzUQusyD3th_La775QKUV && tar -xf abstract_art.tar.gz  # Abstract Art Dataset\n",
    "# !gdown 1VJow74U3H7KG_HOiP1WWo6LoqoE3azJj && tar -xf anime_faces.tar.gz # Anime Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ImageDataModuleConfig:\n",
    "    data_dir: str = \"butterflies256\"\n",
    "    image_size: Tuple[int, int] = (32, 32)\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 8\n",
    "    pin_memory: bool = True\n",
    "    persistent_workers: bool = True\n",
    "\n",
    "\n",
    "class ImageDataModule(LightningDataModule):\n",
    "    def __init__(self, config: ImageDataModuleConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        transform = T.Compose(\n",
    "            [\n",
    "                T.Resize(self.config.image_size),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "                T.Lambda(lambda x: (x * 2) - 1),\n",
    "            ]\n",
    "        )\n",
    "        self.dataset = ImageFolder(self.config.data_dir, transform=transform)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=self.config.pin_memory,\n",
    "            persistent_workers=self.config.persistent_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GroupNorm(channels: int) -> nn.GroupNorm:\n",
    "    return nn.GroupNorm(num_groups=min(32, channels // 4), num_channels=channels)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        n_heads: int = 8,\n",
    "        dropout: float = 0.3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.qkv_projection = nn.Sequential(\n",
    "            GroupNorm(in_channels),\n",
    "            nn.Conv2d(in_channels, 3 * in_channels, kernel_size=1, bias=False),\n",
    "            Rearrange(\"b (i h d) x y -> i b h (x y) d\", i=3, h=n_heads),\n",
    "        )\n",
    "        self.output_projection = nn.Sequential(\n",
    "            Rearrange(\"b h l d -> b l (h d)\"),\n",
    "            nn.Linear(in_channels, out_channels, bias=False),\n",
    "            Rearrange(\"b l d -> b d l\"),\n",
    "            GroupNorm(out_channels),\n",
    "            nn.Dropout1d(dropout),\n",
    "        )\n",
    "        self.residual_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        q, k, v = self.qkv_projection(x).unbind(dim=0)\n",
    "        q = q.contiguous()\n",
    "        k = k.contiguous()\n",
    "        v = v.contiguous()\n",
    "        \n",
    "        output = F.scaled_dot_product_attention(\n",
    "            q, k, v, dropout_p=self.dropout if self.training else 0.0, is_causal=False\n",
    "        )\n",
    "        output = self.output_projection(output)\n",
    "        output = rearrange(output, \"b c (x y) -> b c x y\", x=x.shape[-2], y=x.shape[-1])\n",
    "\n",
    "        return output + self.residual_projection(x)\n",
    "\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        noise_level_channels: int,\n",
    "        dropout: float = 0.3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_projection = nn.Sequential(\n",
    "            GroupNorm(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=\"same\"),\n",
    "            nn.Dropout2d(dropout),\n",
    "        )\n",
    "        self.noise_level_projection = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(noise_level_channels, out_channels, kernel_size=1),\n",
    "        )\n",
    "        self.output_projection = nn.Sequential(\n",
    "            GroupNorm(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=\"same\"),\n",
    "            nn.Dropout2d(dropout),\n",
    "        )\n",
    "        self.residual_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: Tensor, noise_level: Tensor) -> Tensor:\n",
    "        h = self.input_projection(x)\n",
    "        h = h + self.noise_level_projection(noise_level)\n",
    "\n",
    "        return self.output_projection(h) + self.residual_projection(x)\n",
    "\n",
    "\n",
    "class UNetBlockWithSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        noise_level_channels: int,\n",
    "        n_heads: int = 8,\n",
    "        dropout: float = 0.3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.unet_block = UNetBlock(\n",
    "            in_channels, out_channels, noise_level_channels, dropout\n",
    "        )\n",
    "        self.self_attention = SelfAttention(\n",
    "            out_channels, out_channels, n_heads, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, noise_level: Tensor) -> Tensor:\n",
    "        return self.self_attention(self.unet_block(x, noise_level))\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange(\"b c (h ph) (w pw) -> b (c ph pw) h w\", ph=2, pw=2),\n",
    "            nn.Conv2d(4 * channels, channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2.0, mode=\"nearest\"),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=\"same\"),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class NoiseLevelEmbedding(nn.Module):\n",
    "    def __init__(self, channels: int, scale: float = 16.0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(channels // 2) * scale, requires_grad=False)\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(channels, 4 * channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(4 * channels, channels),\n",
    "            Rearrange(\"b c -> b c () ()\"),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = x[:, None] * self.W[None, :] * 2 * torch.pi\n",
    "        h = torch.cat([torch.sin(h), torch.cos(h)], dim=-1)\n",
    "\n",
    "        return self.projection(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet\n",
    "\n",
    "Our UNet is inspired by Stable Diffusion and Imagen. It's not the same UNet as the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UNetConfig:\n",
    "    channels: int = 3\n",
    "    noise_level_channels: int = 256\n",
    "    noise_level_scale: float = 16.0\n",
    "    n_heads: int = 8\n",
    "    top_blocks_channels: Tuple[int, ...] = (128, 128)\n",
    "    top_blocks_n_blocks_per_resolution: Tuple[int, ...] = (2, 2)\n",
    "    top_blocks_has_resampling: Tuple[bool, ...] = (True, True)\n",
    "    top_blocks_dropout: Tuple[float, ...] = (0.0, 0.0)\n",
    "    mid_blocks_channels: Tuple[int, ...] = (256, 512)\n",
    "    mid_blocks_n_blocks_per_resolution: Tuple[int, ...] = (4, 4)\n",
    "    mid_blocks_has_resampling: Tuple[bool, ...] = (True, False)\n",
    "    mid_blocks_dropout: Tuple[float, ...] = (0.0, 0.0)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, config: UNetConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.input_projection = nn.Conv2d(\n",
    "            config.channels,\n",
    "            config.top_blocks_channels[0],\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.noise_level_embedding = NoiseLevelEmbedding(\n",
    "            config.noise_level_channels, config.noise_level_scale\n",
    "        )\n",
    "        self.top_encoder_blocks = self._make_encoder_blocks(\n",
    "            self.config.top_blocks_channels + self.config.mid_blocks_channels[:1],\n",
    "            self.config.top_blocks_n_blocks_per_resolution,\n",
    "            self.config.top_blocks_has_resampling,\n",
    "            self.config.top_blocks_dropout,\n",
    "            self._make_top_block,\n",
    "        )\n",
    "        self.mid_encoder_blocks = self._make_encoder_blocks(\n",
    "            self.config.mid_blocks_channels + self.config.mid_blocks_channels[-1:],\n",
    "            self.config.mid_blocks_n_blocks_per_resolution,\n",
    "            self.config.mid_blocks_has_resampling,\n",
    "            self.config.mid_blocks_dropout,\n",
    "            self._make_mid_block,\n",
    "        )\n",
    "        self.mid_decoder_blocks = self._make_decoder_blocks(\n",
    "            self.config.mid_blocks_channels + self.config.mid_blocks_channels[-1:],\n",
    "            self.config.mid_blocks_n_blocks_per_resolution,\n",
    "            self.config.mid_blocks_has_resampling,\n",
    "            self.config.mid_blocks_dropout,\n",
    "            self._make_mid_block,\n",
    "        )\n",
    "        self.top_decoder_blocks = self._make_decoder_blocks(\n",
    "            self.config.top_blocks_channels + self.config.mid_blocks_channels[:1],\n",
    "            self.config.top_blocks_n_blocks_per_resolution,\n",
    "            self.config.top_blocks_has_resampling,\n",
    "            self.config.top_blocks_dropout,\n",
    "            self._make_top_block,\n",
    "        )\n",
    "        self.output_projection = nn.Conv2d(\n",
    "            config.top_blocks_channels[0],\n",
    "            config.channels,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, noise_level: Tensor) -> Tensor:\n",
    "        h = self.input_projection(x)\n",
    "        noise_level = self.noise_level_embedding(noise_level)\n",
    "\n",
    "        top_encoder_embeddings = []\n",
    "        for block in self.top_encoder_blocks:\n",
    "            if isinstance(block, UNetBlock):\n",
    "                h = block(h, noise_level)\n",
    "                top_encoder_embeddings.append(h)\n",
    "            else:\n",
    "                h = block(h)\n",
    "\n",
    "        mid_encoder_embeddings = []\n",
    "        for block in self.mid_encoder_blocks:\n",
    "            if isinstance(block, UNetBlockWithSelfAttention):\n",
    "                h = block(h, noise_level)\n",
    "                mid_encoder_embeddings.append(h)\n",
    "            else:\n",
    "                h = block(h)\n",
    "\n",
    "        for block in self.mid_decoder_blocks:\n",
    "            if isinstance(block, UNetBlockWithSelfAttention):\n",
    "                h = torch.cat((h, mid_encoder_embeddings.pop()), dim=1)\n",
    "                h = block(h, noise_level)\n",
    "            else:\n",
    "                h = block(h)\n",
    "\n",
    "        for block in self.top_decoder_blocks:\n",
    "            if isinstance(block, UNetBlock):\n",
    "                h = torch.cat((h, top_encoder_embeddings.pop()), dim=1)\n",
    "                h = block(h, noise_level)\n",
    "            else:\n",
    "                h = block(h)\n",
    "\n",
    "        return self.output_projection(h)\n",
    "\n",
    "    def _make_encoder_blocks(\n",
    "        self,\n",
    "        channels: Tuple[int, ...],\n",
    "        n_blocks_per_resolution: Tuple[int, ...],\n",
    "        has_resampling: Tuple[bool, ...],\n",
    "        dropout: Tuple[float, ...],\n",
    "        block_fn: Callable[[], nn.Module],\n",
    "    ) -> nn.ModuleList:\n",
    "        blocks = nn.ModuleList()\n",
    "\n",
    "        channel_pairs = list(zip(channels[:-1], channels[1:]))\n",
    "        for idx, (in_channels, out_channels) in enumerate(channel_pairs):\n",
    "            for _ in range(n_blocks_per_resolution[idx]):\n",
    "                blocks.append(block_fn(in_channels, out_channels, dropout[idx]))\n",
    "                in_channels = out_channels\n",
    "\n",
    "            if has_resampling[idx]:\n",
    "                blocks.append(Downsample(out_channels))\n",
    "\n",
    "        return blocks\n",
    "\n",
    "    def _make_decoder_blocks(\n",
    "        self,\n",
    "        channels: Tuple[int, ...],\n",
    "        n_blocks_per_resolution: Tuple[int, ...],\n",
    "        has_resampling: Tuple[bool, ...],\n",
    "        dropout: Tuple[float, ...],\n",
    "        block_fn: Callable[[], nn.Module],\n",
    "    ) -> nn.ModuleList:\n",
    "        blocks = nn.ModuleList()\n",
    "\n",
    "        channel_pairs = list(zip(channels[:-1], channels[1:]))[::-1]\n",
    "        for idx, (out_channels, in_channels) in enumerate(channel_pairs):\n",
    "            if has_resampling[::-1][idx]:\n",
    "                blocks.append(Upsample(in_channels))\n",
    "\n",
    "            inner_blocks = []\n",
    "            for _ in range(n_blocks_per_resolution[::-1][idx]):\n",
    "                inner_blocks.append(\n",
    "                    block_fn(in_channels * 2, out_channels, dropout[::-1][idx])\n",
    "                )\n",
    "                out_channels = in_channels\n",
    "            blocks.extend(inner_blocks[::-1])\n",
    "\n",
    "        return blocks\n",
    "\n",
    "    def _make_top_block(\n",
    "        self, in_channels: int, out_channels: int, dropout: float\n",
    "    ) -> UNetBlock:\n",
    "        return UNetBlock(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            self.config.noise_level_channels,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "    def _make_mid_block(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        dropout: float,\n",
    "    ) -> UNetBlockWithSelfAttention:\n",
    "        return UNetBlockWithSelfAttention(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            self.config.noise_level_channels,\n",
    "            self.config.n_heads,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "    def save_pretrained(self, pretrained_path: str) -> None:\n",
    "        os.makedirs(pretrained_path, exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(pretrained_path, \"config.json\"), mode=\"w\") as f:\n",
    "            json.dump(asdict(self.config), f)\n",
    "\n",
    "        torch.save(self.state_dict(), os.path.join(pretrained_path, \"model.pt\"))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_path: str) -> \"UNet\":\n",
    "        with open(os.path.join(pretrained_path, \"config.json\"), mode=\"r\") as f:\n",
    "            config_dict = json.load(f)\n",
    "        config = UNetConfig(**config_dict)\n",
    "\n",
    "        model = cls(config)\n",
    "\n",
    "        state_dict = torch.load(\n",
    "            os.path.join(pretrained_path, \"model.pt\"), map_location=torch.device(\"cpu\")\n",
    "        )\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "summary(UNet(UNetConfig()), input_size=((1, 3, 32, 32), (1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LitUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LitConsistencyModelConfig:\n",
    "    initial_ema_decay_rate: float = 0.95\n",
    "    student_model_ema_decay_rate: float = 0.99993\n",
    "    lr: float = 1e-4\n",
    "    betas: Tuple[float, float] = (0.9, 0.995)\n",
    "    lr_scheduler_start_factor: float = 1e-5\n",
    "    lr_scheduler_iters: int = 10_000\n",
    "    sample_every_n_steps: int = 10_000\n",
    "    num_samples: int = 8\n",
    "    sampling_sigmas: Tuple[Tuple[int, ...], ...] = (\n",
    "        (80,),\n",
    "        (80.0, 0.661),\n",
    "        (80.0, 24.4, 5.84, 0.9, 0.661),\n",
    "    )\n",
    "\n",
    "\n",
    "class LitConsistencyModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        consistency_training: ConsistencyTraining,\n",
    "        consistency_sampling: ConsistencySamplingAndEditing,\n",
    "        student_model: UNet,\n",
    "        teacher_model: UNet,\n",
    "        ema_student_model: UNet,\n",
    "        config: LitConsistencyModelConfig,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.consistency_training = consistency_training\n",
    "        self.consistency_sampling = consistency_sampling\n",
    "        self.student_model = student_model\n",
    "        self.teacher_model = teacher_model\n",
    "        self.ema_student_model = ema_student_model\n",
    "        self.config = config\n",
    "        self.num_timesteps = self.consistency_training.initial_timesteps\n",
    "\n",
    "        self.lpips = LearnedPerceptualImagePatchSimilarity(net_type=\"alex\")\n",
    "\n",
    "        # Freeze teacher and EMA student models and set to eval mode\n",
    "        for param in self.teacher_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.ema_student_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.teacher_model = self.teacher_model.eval()\n",
    "        self.ema_student_model = self.ema_student_model.eval()\n",
    "\n",
    "    def training_step(self, batch: Union[Tensor, List[Tensor]], batch_idx: int) -> None:\n",
    "        if isinstance(batch, list):\n",
    "            batch = batch[0]\n",
    "\n",
    "        output = self.consistency_training(\n",
    "            self.student_model,\n",
    "            self.teacher_model,\n",
    "            batch,\n",
    "            self.global_step,\n",
    "            self.trainer.max_steps,\n",
    "        )\n",
    "        self.num_timesteps = output.num_timesteps\n",
    "\n",
    "        lpips_loss = self.lpips(\n",
    "            output.predicted.clamp(-1.0, 1.0), output.target.clamp(-1.0, 1.0)\n",
    "        )\n",
    "        overflow_loss = F.mse_loss(\n",
    "            output.predicted, output.predicted.detach().clamp(-1.0, 1.0)\n",
    "        )\n",
    "        loss = lpips_loss + overflow_loss\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "                \"lpips_loss\": lpips_loss,\n",
    "                \"overflow_loss\": overflow_loss,\n",
    "                \"num_timesteps\": output.num_timesteps,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_batch_end(\n",
    "        self, outputs: Any, batch: Union[Tensor, List[Tensor]], batch_idx: int\n",
    "    ) -> None:\n",
    "        # Update teacher model\n",
    "        ema_decay_rate = ema_decay_rate_schedule(\n",
    "            self.num_timesteps,\n",
    "            self.config.initial_ema_decay_rate,\n",
    "            self.consistency_training.initial_timesteps,\n",
    "        )\n",
    "        update_ema_model_(self.teacher_model, self.student_model, ema_decay_rate)\n",
    "        self.log_dict({\"ema_decay_rate\": ema_decay_rate})\n",
    "\n",
    "        # Update EMA student model\n",
    "        update_ema_model_(\n",
    "            self.ema_student_model,\n",
    "            self.student_model,\n",
    "            self.config.student_model_ema_decay_rate,\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            (self.global_step + 1) % self.config.sample_every_n_steps == 0\n",
    "        ) or self.global_step == 0:\n",
    "            self.__sample_and_log_samples(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(\n",
    "            self.student_model.parameters(), lr=self.config.lr, betas=self.config.betas\n",
    "        )\n",
    "        sched = torch.optim.lr_scheduler.LinearLR(\n",
    "            opt,\n",
    "            start_factor=self.config.lr_scheduler_start_factor,\n",
    "            total_iters=self.config.lr_scheduler_iters,\n",
    "        )\n",
    "        sched = {\"scheduler\": sched, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [opt], [sched]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __sample_and_log_samples(self, batch: Union[Tensor, List[Tensor]]) -> None:\n",
    "        if isinstance(batch, list):\n",
    "            batch = batch[0]\n",
    "\n",
    "        # Ensure the number of samples does not exceed the batch size\n",
    "        num_samples = min(self.config.num_samples, batch.shape[0])\n",
    "        noise = torch.randn_like(batch[:num_samples])\n",
    "\n",
    "        # Log ground truth samples\n",
    "        self.__log_images(\n",
    "            batch[:num_samples].detach().clone(), f\"ground_truth\", self.global_step\n",
    "        )\n",
    "\n",
    "        for sigmas in self.config.sampling_sigmas:\n",
    "            samples = self.consistency_sampling(\n",
    "                self.ema_student_model, noise, sigmas, clip_denoised=True, verbose=True\n",
    "            )\n",
    "            samples = samples.clamp(min=-1.0, max=1.0)\n",
    "\n",
    "            # Generated samples\n",
    "            self.__log_images(\n",
    "                samples,\n",
    "                f\"generated_samples-sigmas={sigmas}\",\n",
    "                self.global_step,\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __log_images(self, images: Tensor, title: str, global_step: int) -> None:\n",
    "        images = images.detach().float()\n",
    "\n",
    "        grid = make_grid(\n",
    "            images.clamp(-1.0, 1.0), value_range=(-1.0, 1.0), normalize=True\n",
    "        )\n",
    "        self.logger.experiment.add_image(title, grid, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_dm_config: ImageDataModuleConfig\n",
    "    unet_config: UNetConfig\n",
    "    consistency_training: ConsistencyTraining\n",
    "    consistency_sampling: ConsistencySamplingAndEditing\n",
    "    lit_cm_config: LitConsistencyModelConfig\n",
    "    trainer: Trainer\n",
    "    seed: int = 42\n",
    "    model_ckpt_path: str = \"checkpoints/cm\"\n",
    "    resume_ckpt_path: Optional[str] = None\n",
    "\n",
    "\n",
    "def run_training(config: TrainingConfig) -> None:\n",
    "    # Set seed\n",
    "    seed_everything(config.seed)\n",
    "\n",
    "    # Create data module\n",
    "    dm = ImageDataModule(config.image_dm_config)\n",
    "\n",
    "    # Create student and teacher models and EMA student model\n",
    "    student_model = UNet(config.unet_config)\n",
    "    teacher_model = UNet(config.unet_config)\n",
    "    teacher_model.load_state_dict(student_model.state_dict())\n",
    "    ema_student_model = UNet(config.unet_config)\n",
    "    ema_student_model.load_state_dict(student_model.state_dict())\n",
    "\n",
    "    # Create lightning module\n",
    "    lit_cm = LitConsistencyModel(\n",
    "        config.consistency_training,\n",
    "        config.consistency_sampling,\n",
    "        student_model,\n",
    "        teacher_model,\n",
    "        ema_student_model,\n",
    "        config.lit_cm_config,\n",
    "    )\n",
    "\n",
    "    # Run training\n",
    "    config.trainer.fit(lit_cm, dm, ckpt_path=config.resume_ckpt_path)\n",
    "\n",
    "    # Save model\n",
    "    lit_cm.ema_student_model.save_pretrained(config.model_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** While the paper suggests a final_timesteps value of `150`, it's important to note that during my experimentation on a smaller dataset with approximately 10,000 training steps, I discovered that setting final_timesteps to `17` yielded superior results. This value corresponds to the number of timesteps that would be obtained if we were to train for the same number of steps as mentioned in the paper (600,000 to 1,000,000). However, it's crucial to emphasize that this particular setting has not been extensively explored or experimented with, so it may require further fine-tuning and adjustment to suit your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type                                  | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | student_model     | UNet                                  | 128 M \n",
      "1 | teacher_model     | UNet                                  | 128 M \n",
      "2 | ema_student_model | UNet                                  | 128 M \n",
      "3 | lpips             | LearnedPerceptualImagePatchSimilarity | 2.5 M \n",
      "----------------------------------------------------------------------------\n",
      "128 M     Trainable params\n",
      "260 M     Non-trainable params\n",
      "389 M     Total params\n",
      "1,556.835 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62aae197ee154de68f9e5d6a49ba5999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwgu/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "query: last dimension must be contiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_config \u001b[39m=\u001b[39m TrainingConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     image_dm_config\u001b[39m=\u001b[39mImageDataModuleConfig(\u001b[39m\"\u001b[39m\u001b[39mbutterflies256\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     unet_config\u001b[39m=\u001b[39mUNetConfig(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m run_training(training_config)\n",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m lit_cm \u001b[39m=\u001b[39m LitConsistencyModel(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     config\u001b[39m.\u001b[39mconsistency_training,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     config\u001b[39m.\u001b[39mconsistency_sampling,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     config\u001b[39m.\u001b[39mlit_cm_config,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Run training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m config\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mfit(lit_cm, dm, ckpt_path\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mresume_ckpt_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Save model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m lit_cm\u001b[39m.\u001b[39mema_student_model\u001b[39m.\u001b[39msave_pretrained(config\u001b[39m.\u001b[39mmodel_ckpt_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(model, ckpt_path\u001b[39m=\u001b[39mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m   1036\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_loop\u001b[39m.\u001b[39mrun(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:260\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    259\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_batch_end\u001b[39m\u001b[39m\"\u001b[39m, batch_output, batch, batch_idx)\n\u001b[0;32m--> 260\u001b[0m call\u001b[39m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_batch_end\u001b[39m\u001b[39m\"\u001b[39m, batch_output, batch, batch_idx)\n\u001b[1;32m    261\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_batch_end()\n\u001b[1;32m    263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m update_ema_model_(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mema_student_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstudent_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mstudent_model_ema_decay_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msample_every_n_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m ) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__sample_and_log_samples(batch)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_images(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     batch[:num_samples]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mclone(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mground_truth\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39mfor\u001b[39;00m sigmas \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msampling_sigmas:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconsistency_sampling(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mema_student_model, noise, sigmas, clip_denoised\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m     samples \u001b[39m=\u001b[39m samples\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     \u001b[39m# Generated samples\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/consistency_models-0.0.3-py3.11.egg/consistency_models/consistency_models.py:639\u001b[0m, in \u001b[0;36mConsistencySamplingAndEditing.__call__\u001b[0;34m(self, model, y, sigmas, mask, transform_fn, inverse_transform_fn, start_from_y, add_initial_noise, clip_denoised, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m x \u001b[39m=\u001b[39m y \u001b[39m+\u001b[39m sigmas[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(y) \u001b[39mif\u001b[39;00m add_initial_noise \u001b[39melse\u001b[39;00m y\n\u001b[1;32m    638\u001b[0m sigma \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), sigmas[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 639\u001b[0m x \u001b[39m=\u001b[39m model_forward_wrapper(\n\u001b[1;32m    640\u001b[0m     model, x, sigma, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma_min, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    641\u001b[0m )\n\u001b[1;32m    642\u001b[0m \u001b[39mif\u001b[39;00m clip_denoised:\n\u001b[1;32m    643\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/consistency_models-0.0.3-py3.11.egg/consistency_models/consistency_models.py:305\u001b[0m, in \u001b[0;36mmodel_forward_wrapper\u001b[0;34m(model, x, sigma, sigma_data, sigma_min, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m c_skip \u001b[39m=\u001b[39m pad_dims_like(c_skip, x)\n\u001b[1;32m    303\u001b[0m c_out \u001b[39m=\u001b[39m pad_dims_like(c_out, x)\n\u001b[0;32m--> 305\u001b[0m \u001b[39mreturn\u001b[39;00m c_skip \u001b[39m*\u001b[39m x \u001b[39m+\u001b[39m c_out \u001b[39m*\u001b[39m model(x, sigma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 28\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmid_encoder_blocks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(block, UNetBlockWithSelfAttention):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         h \u001b[39m=\u001b[39m block(h, noise_level)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         mid_encoder_embeddings\u001b[39m.\u001b[39mappend(h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 28\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, noise_level: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet_block(x, noise_level))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     q, k, v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqkv_projection(x)\u001b[39m.\u001b[39munbind(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         q, k, v, dropout_p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39melse\u001b[39;00m \u001b[39m0.0\u001b[39m, is_causal\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_projection(output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X36sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     output \u001b[39m=\u001b[39m rearrange(output, \u001b[39m\"\u001b[39m\u001b[39mb c (x y) -> b c x y\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], y\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: query: last dimension must be contiguous"
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(\n",
    "    image_dm_config=ImageDataModuleConfig(\"butterflies256\"),\n",
    "    unet_config=UNetConfig(),\n",
    "    consistency_training=ConsistencyTraining(final_timesteps=17),\n",
    "    consistency_sampling=ConsistencySamplingAndEditing(),\n",
    "    lit_cm_config=LitConsistencyModelConfig(\n",
    "        sample_every_n_steps=1000, lr_scheduler_iters=1000\n",
    "    ),\n",
    "    trainer=Trainer(\n",
    "        max_steps=10_000,\n",
    "        precision=\"16-mixed\",\n",
    "        log_every_n_steps=10,\n",
    "        logger=TensorBoardLogger(\".\", name=\"logs\", version=\"cm\"),\n",
    "        callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
    "    ),\n",
    ")\n",
    "run_training(training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ² Sampling & Zero-shot Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images: Tensor, cols: int = 4) -> None:\n",
    "    rows = max(images.shape[0] // cols, 1)\n",
    "    fig, axs = plt.subplots(rows, cols)\n",
    "    axs = axs.flatten()\n",
    "    for i, image in enumerate(images):\n",
    "        axs[i].imshow(image.permute(1, 2, 0).numpy() / 2 + 0.5)\n",
    "        axs[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/cm/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 34\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbfloat16 \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfloat32\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m unet \u001b[39m=\u001b[39m UNet\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mcheckpoints/cm\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X45sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X45sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUNet\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X45sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pretrained_path, \u001b[39m\"\u001b[39m\u001b[39mconfig.json\u001b[39m\u001b[39m\"\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X45sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m         config_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X45sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m     config \u001b[39m=\u001b[39m UNetConfig(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_dict)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/cm/config.json'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "unet = UNet.from_pretrained(\"checkpoints/cm\").eval().to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = ImageDataModule(ImageDataModuleConfig(\"butterflies256\", batch_size=4))\n",
    "dm.setup()\n",
    "\n",
    "batch, _ = next(iter(dm.train_dataloader()))\n",
    "batch = batch.to(device=device, dtype=dtype)\n",
    "\n",
    "plot_images(batch.float().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb Cell 39\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m consistency_sampling_and_editing \u001b[39m=\u001b[39m ConsistencySamplingAndEditing()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     samples \u001b[39m=\u001b[39m consistency_sampling_and_editing(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         unet,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         torch\u001b[39m.\u001b[39mrandn((\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m), device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         sigmas\u001b[39m=\u001b[39m[\u001b[39m80.0\u001b[39m],  \u001b[39m# Use more steps for better samples e.g 2-5\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         clip_denoised\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jwgu/git/consistency_models/notebooks/consistency_models_training_example.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m plot_images(samples\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcpu())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unet' is not defined"
     ]
    }
   ],
   "source": [
    "consistency_sampling_and_editing = ConsistencySamplingAndEditing()\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = consistency_sampling_and_editing(\n",
    "        unet,\n",
    "        torch.randn((4, 3, 32, 32), device=device, dtype=dtype),\n",
    "        sigmas=[80.0],  # Use more steps for better samples e.g 2-5\n",
    "        clip_denoised=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "plot_images(samples.float().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_erasing = T.RandomErasing(p=1.0, scale=(0.2, 0.5), ratio=(0.5, 0.5))\n",
    "masked_batch = random_erasing(batch)\n",
    "mask = torch.logical_not(batch == masked_batch)\n",
    "\n",
    "plot_images(masked_batch.float().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inpainted_batch = consistency_sampling_and_editing(\n",
    "        unet,\n",
    "        masked_batch,\n",
    "        sigmas=[5.23, 2.25],\n",
    "        mask=mask.to(dtype=dtype),\n",
    "        clip_denoised=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "plot_images(torch.cat((masked_batch, inpainted_batch), dim=0).float().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_a = batch.clone()\n",
    "batch_b = torch.flip(batch, dims=(0,))\n",
    "\n",
    "plot_images(torch.cat((batch_a, batch_b), dim=0).float().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    interpolated_batch = consistency_sampling_and_editing.interpolate(\n",
    "        unet,\n",
    "        batch_a,\n",
    "        batch_b,\n",
    "        ab_ratio=0.5,\n",
    "        sigmas=[5.23, 2.25],\n",
    "        clip_denoised=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "plot_images(torch.cat((batch_a, batch_b, interpolated_batch), dim=0).float().cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
